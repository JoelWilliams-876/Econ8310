{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872b72f9-e22e-4d24-b2af-eb92daa3c388",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Let's learn about our first statistical modeling technique. Linear regression is sometimes considered the most basic machine learning algorithm. Strictly speaking, this is true; linear regression takes data as an input, and updates its rule set based on that data to model an outcome. This is the basis for machine learning! On the other hand, linear regression as a statistical tool definitely predates the machine learning movement, and has been used for over 200 years. It is well understood, and is readily accepted as a means of analysis.\n",
    "\n",
    "Whether or not you think of it as machine learning, it is a great place to get started when learning about data modeling through statistics. Let's build up some intuition for how linear regression works before we learn to use it.\n",
    "\n",
    "## Cause and Effect\n",
    "\n",
    "The goal of statistical modeling is to understand the inputs that **cause** some specific outcome that we want to study. The big catch with statistical models is statistical models do not successfully identify **causation**. Statistical models instead identify **correlation**, and leave **causation** to domain expertise.\n",
    "\n",
    "- **Correlation**: a mutual relationship or connection between two or more things. In statistics, we even use measures such as the correlation coefficient to describe the intensity of the relationship between two variables. In order for the relationship between two variables to be a **causal** relationship, there must first be **correlation** between those variables.\n",
    "\n",
    "- **Causation**: the act of causing something. This is the relationship that statistical models want to measure! Unfortunately, statistics alone can't get us to **causation**. In order to establish a **causal** relationship, we must combine correlation between two variables with an explanation of *how* one of those variables can lead to changes in the other.\n",
    "\n",
    "## Questioning Causality\n",
    "\n",
    "When we hypothesize a causal relationship (that $x$ causes $y$), it is important to ask ourselves several questions:\n",
    "\n",
    "1. Is it possible that $y$ causes $x$ instead?\n",
    "2. Is it possible that $z$ (a new factor that we haven't considered before) is causing both $x$ and $y$?\n",
    "3. Could the relationship have been observed by chance?\n",
    "\n",
    "If we can demonstrate that each of these questions is unlikely to be answered in the affirmative, and we observe correlation between two variables, then we can begin to assert that the relationship may be causal.\n",
    "\n",
    "## Establishing Causality\n",
    "\n",
    "In order to establish causality, we need to meet several conditions:\n",
    "\n",
    "- We can explain **why** $x$ causes $y$\n",
    "- We can demonstrate that **nothing else is driving the changes** (within reason)\n",
    "- We can show that there is a **correlation** between $x$ and $y$\n",
    "\n",
    "In other words, we need a way to statistically isolate the relationship between two variables, even when there are other \"moving parts\" in our model.\n",
    "\n",
    "## RCT\n",
    "\n",
    "One way to establish causality is through Randomized controlled trials (RCTs). In the context of an RCT, the experiment is designed such that only one variable is experimented upon. By assigning random individuals (or entities, groups, etc.) to the treatment and control groups, the researcher can use univariate statistical tests to determine the relationship between the variable of interest (called the treatment variable) and the outcome (dependent variable).\n",
    "\n",
    "Unfortunately, there are **many** contexts in which creating an RCT is not feasible. This may be due to the data collection method, it may be due to ethical concerns, or some other internal or external factor. Where we cannot perform an RCT, regression analysis becomes our next best option.\n",
    "\n",
    "## Regression Analysis\n",
    "\n",
    "If you have ever created a trend line in Excel or Tableau (or any similar software), then you have implemented a form of regression analysis. The beauty of regression analysis is in its ability to be **more** than just a trend line on a plot. While a trendline is valuable, it is only helpful when describing the relationship between two (and maybe three) variables, regression analysis makes it possible to create the analog to a trendline using **as many variables as you would like** (so long as you have more observations than variables).\n",
    "\n",
    "The underlying concept (we won't go into the math here) is that we want to statistically separate the effect of each variable on the outcome. In other words, what happens to our outcome when we vary only one of our many variables at a time? All of the math behind regression analysis is designed to answer this question. This means that regression analysis\n",
    "\n",
    "- Allows us to **act as if nothing else were changing**\n",
    "- Mathematicaly isolates the effect of each individual **variable** on the outcome of interest\n",
    "    - Variables are the factors that we want to include in our model\n",
    "    \n",
    "When we look at the output of a regression, then, we are looking at an equation that tells us how to add up the impacts of each variable to estimate the value of our dependent variable! Not only can we understand the impact of each individual variable, but we can also forecast the dependent variable for use in predictive modeling.\n",
    "\n",
    "\n",
    "### Regression Terms\n",
    "\n",
    "As we move forward, it will be helpful to keep in mind the following definitions:\n",
    "\n",
    "- **Coefficient**: This is the effect of changing a variable by one unit (from “untreated” to “treated”)\n",
    "- **Standard Error (Standard Deviation)**: Measures how noisy the effect of the independent variable is on the dependent variable\n",
    "    - Larger numbers indicate more noise\n",
    "- **Confidence Interval**: Assuming our regression analysis includes all relevant information, we expect that the true coefficient (treatment effect) lies within this range 95% of the time (for a 95% confidence interval)\n",
    "\n",
    "- **Statistical Significance**: When the Average Treatment Effect has a confidence interval (at 95% or 99% levels, typically) that does not include 0\n",
    "\n",
    "\n",
    "\n",
    "### Regression Assumptions\n",
    "\n",
    "It is important to note that the statistical models underlying linear regression depend on several assumptions:\n",
    "\n",
    "1. Effects are Linear (there are some workarounds)\n",
    "2. Errors are normally distributed (bell-shaped)\n",
    "3. Variables are not Collinear\n",
    "4. No Autocorrelation (problematic for time series data)\n",
    "5. Homoskedasticity (errors are shaped the same across all observations)\n",
    "\n",
    "While the study of these assumptions is essential for a practitioner, and frequently occupies an entire semester-long course, it is sufficient for us to state these assumptions, and be aware of the fact that these assumptions are baked into the models. It is equally important to know that adaptations of regression analysis exist to work around violations of each assumption where needed.\n",
    "\n",
    "Most regressions implemented in the real world must account for violations of one or more assumptions.\n",
    "\n",
    "## When should we use regression, then?\n",
    "\n",
    "Regression Analysis is most useful when you care about WHY a particular outcome occurs. Regressions are very powerful transparent models, by which I mean that it is straightforward to see how each variable leads to the predicted outcome. Whenever we want to establish causality (and can't put together an RCT), regression models are the *de facto* standard for understanding how one variable causes the other to change.\n",
    "\n",
    "If, on the other hand, you want to just predict WHAT will happen next, there exist much better tools for you! We will spend many class sessions discussing these models during the remainder of this course.\n",
    "\n",
    "## Implementing Linear Regression in Python\n",
    "\n",
    "\n",
    "Now let's talk about actually **doing** linear regression. In order to perform regression analysis, we will utilize the `statsmodels` library, which is capable of performing most types of regression modeling. While the library is very robust, we will focus on running linear regression under standard assumptions during this class. Let's start by importing the library and some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd2989-0eb4-4c76-8d8b-a95bb323c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our libraries\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "\n",
    "# Import data to estimate the weight of fish sold at market\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/fishWeight.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27c371-e0b3-47ca-a648-a94daf39f1c1",
   "metadata": {},
   "source": [
    "Note that when we import `statsmodels`, we do so with a slightly different syntax (`import statsmodels.formula.api as smf`) rather than just importing the whole library. `statsmodels` provides the option to import two distinct APIs (application programming interfaces). We are using the formulas API, which will allow us to write intuitive regression equations that more easily permit us to modify our data as we run regressions.\n",
    "\n",
    "Now, we really don't have much work left to do before we can run a regression! Let's look at our data really quickly, and then try out some regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b1f8a-2065-44c4-8ac8-d96a19eeccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3dd0b6-7061-4161-8e18-bf10a4666392",
   "metadata": {},
   "source": [
    "As we can see above, we don't have too many variables. We have three measures of length (two are diagonal measures of the fish), we have height and width, then we have species, and finally, our dependent variable: weight. Let's see how well `Length1` predicts weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7ab1f-71e4-492b-98e9-0a846bd4c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = smf.ols(\"Weight ~ Length1\", data=data)\n",
    "\n",
    "reg = reg.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46774a44-8282-4065-8c0c-2d980ceb67e8",
   "metadata": {},
   "source": [
    "Done! We have just implemented our first regression model! The function `smf.ols` is the function to implement OLS, or Ordinary Least Squares Regression. OLS is what is typically meant when someone says that they are going to use regression analysis, although other kinds of regressions certainly exist.\n",
    "\n",
    "We provide two arguments to our regression function:\n",
    "- A formula for the regression model\n",
    "- The data set\n",
    "\n",
    "Our formula always includes the dependent variable as the leftmost element, and is separated from independent variables by the `~` symbol. After we create our model, we have to use the `.fit()` method to calculate the optimal weights for our regression model.\n",
    "\n",
    "Now, we can call the `.summary()` method on the fitted model in order to see how our model is structured and its anticipated performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e398c1e6-be94-4bb5-94a6-ef424b060375",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e56ef0-b121-4b66-a07e-55f29ada0a18",
   "metadata": {},
   "source": [
    "The most important features on this regression table are the `R-squared` and its adjusted form, as well as the table of coefficients and standard errors. In the above model, our $R^2$ is about .83, indicating that our model (including only an intercept term and the length of the fish) explains 83% of the variation in weight of the fish! That is pretty good! But I bet we can do better if we include more terms in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d9c8af-9bda-4ccb-a069-9da7c0a349bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = smf.ols(\"Weight ~ Length1 + C(Species)\", data=data)\n",
    "\n",
    "reg = reg.fit()\n",
    "\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea860ff2-4944-4c72-8217-2f463853759d",
   "metadata": {},
   "source": [
    "When we add a new independent variable to our regression model, we separate each independent variable from the others using the `+` symbol. Additionally, we can use very simple syntax to include categorical variables by wrapping a variable name in the `C()` syntax. That variable is then automatically separated into separate binary variables for each class within the column!\n",
    "\n",
    "Adding this single variable increased our $R^2$ from .83 to .93!\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "Linear regression is typically used to describe outcomes in which the value is continuous. This could be a model of the amount of profit generated through various business practices, the population density of a region, or any other metric that can be measured on a more or less continuous scale. Less frequently, linear regression is used to model outcomes that represent discrete outcomes such as success or failure, or to model the probability of success or failure. This is called a **linear probability model (LPM)**.\n",
    "\n",
    "Why might LPMs not be used very often? Let's think about probability, as well as the properties of a linear regression model. A probability is a measure of likelihood, and is typically measured on the scale of $[0,1]$, or from 0% to 100% probability. A probability of 0 (0%) indicates that there is no likelihood that an event will take place. On the other hand, a probability of 1 (100%) would suggest absolute certainty that an outcome will occur. The important point, no matter which measurement of probability we choose to use, is that probabilities are **bounded** (cannot go beyond) absolute certainty of failure or absolute certainty of success.\n",
    "\n",
    "Remember linear regression? One important part of any linear regression model is the **linearity** of the model. While this seems obvious, it is this part of the linear regression that gets us in trouble as we use LPMs. Any linear function with a nonzero slope will by definition be **unbounded**, and will NOT remain within the $[0, 1]$ interval. This means that an LPM will inevitably provide predictions that are non-sensical! Through our LPM, we will get some predictions that fit into each of the following categories:\n",
    "\n",
    "- Totally normal predictions, with probabilities between 0 and 1\n",
    "- Weird probabilities #1, with probability above 1 (or a likelihood of greater than 100%, which doesn't make sense!)\n",
    "- Weird probabilities #2, with probability below 0 (or a negative likelihood, which again doesn't make sense!)\n",
    "\n",
    "## Non-Linear, But in a Good Way\n",
    "\n",
    "Is there a better way to model probabilities using regression analysis? You bet there is! It is called **logistic regression**, and we will learn how to do it, right here, right now. But first, let's explore why it is an improvement over LPMs. Remember that the big problem with LPMs is their linearity, right? It is the part of a regression that makes regression analysis so simple to understand, but also the part that breaks our probability model. We want to redesign our regression model to **resemble** a linear model, but stay within the $[0, 1]$ interval.\n",
    "\n",
    "When we use a linear regression model, we end up with a series of coefficients. Those are the slope parameters for a linear equation resulting in our prediction of the dependent variable (outcome). We typically refer to each coefficient as a $\\beta$ (beta) term, with subscripts ($\\beta_i$) denoting which coefficient we are referring to. If we use the same subscripts to describe our $x$ variables, then we can write our regression equation (with $k$ variables) as follows:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 ... + \\beta_k \\cdot x_k$$\n",
    "\n",
    "Obviously, as our $x$ values increase, our $y$ either increases or decreases, depending on the sign and magnitude of the associated $\\beta$ coefficient. If those $x$'s get sufficiently large, so does $y$! In fact, it can go as high as $\\infty$ and as low as $-\\infty$. This is a problem now we are dealing with probability.\n",
    "\n",
    "To fix our regression equation, we make a really simple transformation called the **logistic transformation**. After the transformation, our regression equation is written as follows:\n",
    "\n",
    "$$ y = \\frac{exp(\\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 ... + \\beta_k \\cdot x_k)}{1+ exp(\\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 ... + \\beta_k \\cdot x_k)} $$\n",
    "\n",
    "where $exp()$ represents Euler's number raised to the power of the internal element (in our case, our original linear regression function).\n",
    "\n",
    "Why do we choose this function? Several reasons:\n",
    "\n",
    "- It is a simple transformation\n",
    "- It leads to interpretable coefficients (remember that we want those!)\n",
    "- It is bounded by $[0,1]$ like we want\n",
    "\n",
    "How is it bounded? Remember that our linear regression function can go to either $\\infty$ or $-\\infty$. If the linear function takes the value $\\infty$, then our logistic function becomes\n",
    "\n",
    "$$ y = \\frac{\\infty}{1+\\infty} \\approx 1$$\n",
    "\n",
    "because $exp(\\infty)=\\infty$. When the linear function takes the value $-\\infty$, then $exp(-\\infty)=0$, so our logistic function becomes\n",
    "\n",
    "$$ y = \\frac{0}{1} = 0 $$\n",
    "\n",
    "and we remain within our probability bounds!\n",
    "\n",
    "## Interpreting Coefficients\n",
    "\n",
    "The confusing part of logistic regression stems from understanding the difference between coefficients in linear regression and in logistic regression. In linear regression, a coefficient describes the change in the dependent variable resulting from a one unit **increase** in the independent variable associated with that coefficient. If, for example, the coefficient of age on income (in Euros) is &#128;1,000, then an individual would be expected to earn &#128;1,000 more if he or she were 1 year older, or &#128;1,000 less if he or she were 1 year younger.\n",
    "\n",
    "In a logistic regression, our coefficient is not a linear treatment effect. Instead, coefficients represent the **log odds ratio**, which can be written as\n",
    "\n",
    "$$ \\beta = ln\\left(\\frac{p}{1-p}\\right) $$\n",
    "\n",
    "If the **log odds ratio** is greater than 0, then a one unit increase in the associated variable would lead to an increased likelihood of success in the dependent variable. If the value is below 0, then the likelihood of success diminishes. The effects are not linear, but they do reflect the direction of the trend and can be interpreted to understand the relationship that each variable has with the outcome.\n",
    "\n",
    "## Implementing Logistic Regression\n",
    "\n",
    "Implementing logistic regression is nearly identical to the implementation of linear regression, thanks to the ease of use provided through the `statsmodels` library. Let's import some data and create a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc4385-cbf5-481b-83ea-be24089d3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/roomOccupancy.csv\")\n",
    "\n",
    "reg = smf.logit(\"Occupancy ~ Light + CO2\", data=data).fit()\n",
    "\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45f14c-c738-4243-b337-82ef95fe8500",
   "metadata": {},
   "source": [
    "There are only two differences between this code and the code needed for a linear regression:\n",
    "\n",
    "1. The dependent variable is binary (or exists within the range of 0 to 1)\n",
    "2. We call the `smf.logit` function instead of `smf.ols`\n",
    "\n",
    "We can use the same regression equation syntax to describe our regression model as we did with OLS, and we can use the same functions to fit our model. Do note, however, that the fitting process that occurs behind the scenes is different, and logistic regressions may take a significant amount of time to finish estimation if there are a large number of parameters and/or observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1a83bb-8848-4ec0-b15e-3c9acd830c85",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdafe6c4-725e-4b7d-ad57-bd354cd22fa2",
   "metadata": {},
   "source": [
    "# AutoRegressive Integrated Moving Average Models\n",
    "#### Or ARIMA, for short\n",
    "\n",
    "## You know what they say about assumptions...\n",
    "\n",
    "When we use OLS models, we are making five assumptions, which are often called the **standard assumptions**. These assumptions are required in order to mathematically derive the OLS regression solution. These rules are (according to *Econometric Analysis* by William Greene):\n",
    "\n",
    "1) There must be a linear relationship between the dependent variable and any independent variable (that's why it's called **linear** regression!!)\n",
    "\n",
    "2) Full rank (you can't recreate any $x$ variables through a linear combination of other $x$ variables)\n",
    "\n",
    "3) Errors have mean zero. The error term for one observation $\\epsilon_i$ cannot be predicted by $x$ variables. \n",
    "\n",
    "4) Every error term $\\epsilon_i$ is unrelated to any other error term $\\epsilon_j$, and each error term has the same variance $\\sigma$. Violations of these combined conditions are called autocorrelation and heteroskedasticity, respectively. Autocorrelation, in particular, is crucial in a time-series context.\n",
    "\n",
    "5) Errors are normally distributed. Probably the easiest assumption to violate. Kind of like a speeding ticket...\n",
    "\n",
    "<br>\n",
    "\n",
    "While these assumptions are important, they are NOT required in order to perform regression! In fact, they are often more important (and interesting) in the usefulness of their violations (and the solutions to those new models) than they are in a list like this one. It's just important that you know they exist before we go breaking them and making newer and more exciting models.\n",
    "\n",
    "### More like guidelines than real rules\n",
    "\n",
    "In many (most?) cases, we want to work with data that demonstrably violate some of these assumptions. That is fine! Just a few points, though...\n",
    "- OLS is no longer guaranteed to model \"truth\" once the assumptions are violated (that seems bad...)\n",
    "- There are models that have been developed to deal with nearly every possible way of violating these assumptions\n",
    "- We will discuss those that are most relevant to forecasting (yay!)\n",
    "\n",
    "<br>\n",
    "\n",
    "Now that we know this, we just need to figure out whether time-series data (the kind of data we are going to focus on in the first half of this course) violates any of these rules. Knowing what is broken enables us to focus on finding a way to model our data that does not depend on the violated assumption. \n",
    "\n",
    "### So how about time-series data?\n",
    "\n",
    "Let's start by understanding what time-series data actually is. Strictly speaking, time-series data is data focused on a **single variable**, and tracking the value of that variable over time. We also frequently call data time-series data if it is a collection of variables tracked over time. Let's take a look at some time-series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d5f39-f7cb-4113-8253-635ffff24d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# The full data set is ~30 Mb so this might not be fast...\n",
    "# Grab the last year of the data\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8310/raw/master/DataSets/omahaNOAA.csv\")\n",
    "# Clean it up\n",
    "data = data.loc[len(data)-365*24:, ['DATE', 'HOURLYDRYBULBTEMPC']]\n",
    "data.columns = ['date', 'temp_c']\n",
    "data = data.loc[data['temp_c']!=0] # temp=0 is a 'missing value', which is annoying but fixable\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "# And plot it\n",
    "px.scatter(data, x='date', y='temp_c')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd917b6f-3439-4338-9a5c-9380c5a924a1",
   "metadata": {},
   "source": [
    "What is the first thing you notice about this plotted data? Does it help if we look at a smaller subset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f13ee-f135-475f-91f1-dbdea85388aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(data[-100:], x='date', y='temp_c')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d56db-50ae-428e-b199-8b20ffd16701",
   "metadata": {
    "tags": []
   },
   "source": [
    "What stands out to me about this data is that there is a **pattern** from one observation to the next. This might seem obvious, but it is a really important point about time-series data. Whenever our data is more than noise that has been sampled over time, our time-series data will have a pattern.\n",
    "\n",
    "In math speak, we might say something like\n",
    "\n",
    "$$ Corr(x_t, x_{t+1}) \\neq 0 $$\n",
    "\n",
    "This is a lot like what we assumed does NOT happen in our data under assumption (4) earlier! We can also write this relationship in a way that helps us to start understanding how we might implement regression with time-series data:\n",
    "\n",
    "$$ y_{t} = \\rho \\cdot y_{t-1} + \\epsilon_t $$\n",
    "\n",
    "Where $\\rho$ is the correlation between one observation and the next, and $\\epsilon_t$ is the error or noise term.\n",
    "\n",
    "Let's describe this in plain English. When we work with time-series data, we frequently observe that one observation is correlated with the next observation in the sequence. Because observations are correlated, our data is **not** independent and identically distributed, and therefore the standard assumptions of OLS **do not hold**. Without the standard assumptions OLS is no longer assured to represent our best approximation of truth. We can do better.\n",
    "\n",
    "## Upgrading OLS for time-series\n",
    "\n",
    "One of the best ways to account for violations of the standard assumptions is to eliminate the violation of the assumption from our data, and then use OLS. We are going to construct a new model that will do exactly that in order to deal with time-series data. This will enable us to take advantage of the interpretability of OLS models, while also using more interesting data to make forecasts.\n",
    "\n",
    "### AutoRegressive Models\n",
    "\n",
    "Our weather data is clearly correlated from one period to the next. The temperature in an hour is highly correlated with the temperature right now. The temperature tomorrow is also correlated (less strongly) with the current temperature. This suggests that the best way to describe our data-generating process is with the equation from above:\n",
    "\n",
    "$$ y_{t} = \\rho \\cdot y_{t-1} + \\epsilon_t $$\n",
    "\n",
    "Which also **implicitly** mandates that there be correlation between the current period and **all past time periods**:\n",
    "\n",
    "$$ y_{t} = \\rho \\cdot y_{t-1} + \\epsilon_t = \\rho \\cdot (\\rho \\cdot y_{t-2}) + \\epsilon_t $$\n",
    "$$ = \\rho \\cdot (\\rho \\cdot (\\rho \\cdot y_{t-3})) + \\epsilon_t = ... = \\alpha + \\rho^i \\cdot y_{t-i} + \\epsilon_t $$\n",
    "\n",
    "Today's weather is correlated with the weather in every time period that has ever happened before.\n",
    "\n",
    "The solution to this particular problem with our data is to use an __A__uto**R**egressive (AR) model. AR models are specified to contain a chosen number of lagged observations of $y$ as explanatory variables (they become our $x$ variables), and to use those lags to predict the next value of $y$ in the time-series.\n",
    "\n",
    "By choosing the number of lags in an AR model, we are specifying how quickly we expect a time-series to return to its mean value. The fewer lagged terms we include, the quicker we expect the mean reversion to occur. The number of lagged observations is called the **order** of the model, and is denoted ($p$). When we describe models as AR($p$), we say that they are AutoRegressive models of order $p$:\n",
    "\n",
    "$$ AR(p) \\implies y_t = \\alpha + \\sum_{i=1}^p \\rho_i \\cdot y_{t-i} + \\epsilon _t$$\n",
    "\n",
    "In practice we allow $\\rho_i$ to be estimated independently of all other $\\rho$ values. We will wait to estimate AR models until we have a more complete picture of time series data.\n",
    "\n",
    "### Moving Average Models\n",
    "\n",
    "Brace yourselves. The Moving Average (MA) model may look almost exactly like an AR model, but its subtle differences can be very valuable additions to a time-series model.\n",
    "\n",
    "AR models assume that the current value of $y$ is correlated with past values of $y$. In an MA model, $y_t$ is instead correlated with the **past error term** $\\epsilon_{t-1}$. We can express this mathematically:\n",
    "\n",
    "$$ y_t = \\theta \\cdot \\epsilon_{t-1} + \\epsilon_t $$\n",
    "\n",
    "What is the difference? We know that $\\epsilon_t$ is statistical noise. It represents the deviation of truth from our expectations in time $t$. We also know that it has an expected value of zero, so that our expectations should be correct **on average**. I $\\epsilon$ is built from $y$, then how is this different from an AR model?\n",
    "\n",
    "MA models derive their predictions of tomorrow from the errors of today. Because of this, our model **does not** incorporate persistent information from every previous period, like an AR model. The information about deviation from yesterday is sufficient.\n",
    "\n",
    "Like an AR model, we can choose the **order** of our MA model by incorporating additional error terms from past periods into our model. In the case of the MA model, the order is denoted $q$.\n",
    "\n",
    "$$ MA(q) \\implies y_t = \\alpha + \\sum_{i=1}^q \\theta_i \\cdot \\epsilon_{t-i} + \\epsilon_t $$\n",
    "\n",
    "Because errors are uncorrelated with one another, we will estimate each $\\theta$ term independently in our model. We just need to discuss one more building block before we start building an actual model!\n",
    "\n",
    "## Integration in time-series\n",
    "\n",
    "\n",
    "One of the most pervasive problems in time-series data is a time **trend**. I mean, isn't that kind of what we are hoping to find: a pattern that explains the movement in the series? It turns out that a time-series with a time trend is what we call non-stationary, with stationarity being an important element of assumption 4. A non-stationary model is one with non-uniform mean or variance over time (ie - across observations).\n",
    "\n",
    "The good news is that this is an easy problem to fix. We can remove the time-trend from a model by using differenced (integrated) models. The integration term ($d$) denotes the number of differences that must be taken before a series is considered stationary. Typically we will only consider the cases in which $d \\in \\{1,2\\}$.\n",
    "\n",
    "When $d=1$:\n",
    "\n",
    "$$ \\bar{y}_t = y_t - y_{t-1}$$\n",
    "\n",
    "When $d=2$\n",
    "\n",
    "$$ \\bar{y}_t = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2})$$\n",
    "\n",
    "so that second degree integrated models resemble a difference-in-differences model or second derivative rather than simply a subtration of two previous periods.\n",
    "\n",
    "# The ARIMA Model\n",
    "\n",
    "As statisticians/economists/analysts, we put these three common time-series problems together to form one of the most-used time-series models around: the Auto Regressive Integrated Moving Average (ARIMA) model. \n",
    "\n",
    "An ARIMA model is said to have order $(p, d, q)$, representing the orders of the contained Autoregressive, Integration, and Moving Average parameters, respetively. We will be able to adjust these parameters as we design our model in order to optimize our ability to both forecase and conduct inferential analysis.\n",
    "\n",
    "Let's use `statsmodels` to generate an ARIMA model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b5e504-82a1-42ab-9f34-1ae3b7f84e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Can't have missing data, but also don't want to drop hours, so we will\n",
    "#   fill the data with last known temperature as our best guess of missing\n",
    "#   data\n",
    "data=data.fillna(method='pad')\n",
    "\n",
    "# Implementing an ARIMA(1,0,0) model\n",
    "arima = sm.tsa.ARIMA(data['temp_c'], order=(1, 0, 0)).fit()\n",
    "arima.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a095b-1c68-4b0e-b89d-670cd22210e5",
   "metadata": {},
   "source": [
    "Just like in any `statsmodels` regression, we are able to quickly generate a summary table based on our regression model. We can experiment with different `order` parameters to determine the ideal model for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85e4a7-f933-4898-8a64-d8c06d43746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima = sm.tsa.ARIMA(data['temp_c'], order=(3, 1, 0)).fit()\n",
    "arima.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f2b946-01ea-4c79-bd33-f1c0e84ced19",
   "metadata": {},
   "source": [
    "In addition to simply generating a regression summary table, we can also create forecasts using our ARIMA model, and can even plot those observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f604ef7-f842-487e-8919-092c77e9a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Generate forecast of next 10 hours\n",
    "\n",
    "fcast, sd, cint = arima.forecast(steps=10)\n",
    "\n",
    "# Generate data frame based on forecast\n",
    "times = [data.iloc[-1, 0] + timedelta(hours=i) for i in range(1, 11)]\n",
    "\n",
    "forecast = pd.DataFrame([times, fcast]).T\n",
    "forecast.columns = data.columns = ['date', 'temp_c']\n",
    "\n",
    "# Plot forecast with original data\n",
    "\n",
    "fig = px.line(data[-100:], x='date', y='temp_c')\n",
    "fig.add_trace(go.Scatter(x=forecast[\"date\"], y=forecast[\"temp_c\"], mode='markers', name='Forecast'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902b8dc-0ba3-4f53-9047-6e23201f33ea",
   "metadata": {},
   "source": [
    "# Correctly Diagnosing a Time Series\n",
    "\n",
    "When attempting to determine the best time series model, there are two pathways taht we can take to find the optimal parameters. The first option is to use information criteria (AIC, BIC, etc.) to select a model with acceptable parameters. These criteria tend to be blindly fit to the data, and do not allow us to bring any sort of business understanding to bear in combination to statistical techniques. \n",
    "\n",
    "For this reason, I strongly prefer the second method of creating ARIMA models: visual diagnosis. When we diagnose a model visually, we are able to use information resulting from the data to inform our decision. More importantly, we retain the ability to actively incorporate our understanding of the data itself into our choice. As practitioners, this is critical! The most important task that a data analyst has is to incorporate an understanding of the data into the choices made when optimizing a model for forecasting or prediction.\n",
    "\n",
    "Do NOT be a data scientist who ignores the domain in which the data exists!\n",
    "\n",
    "## The Autocorrelation Function (ACF)\n",
    "\n",
    "When we create our model specification, we will use various visual characteristics of our data to determine the correct specification of the model. The first of these is the Autocorrelation Function (ACF). First things first, let's plot an ACF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97636093-b9fb-45a1-ac37-cd26a5bc44b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8310/blob/master/DataSets/pollutionBeijing.csv?raw=true\").dropna()\n",
    "\n",
    "fig = plot_acf(data['pm2.5'], lags=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df06c564-7aea-45ed-ba03-b09043127206",
   "metadata": {},
   "source": [
    "What does this graph tell us? An ACF plot shows us the average relationship between an observation and various lags of that same variable. In the figure above, we have specified that we want to only see the correlation between an observation and the first 10 lags of that variable. The figure shows very high correlation for the first lag, with slowly tapering correlation for each subsequent lagged period. \n",
    "\n",
    "When we see this type of pattern, it is an indication that there is an AR process at work in our time series. Our response to this figure should be to increase the AR order of our model (increase the $p$ parameter by one or more), to account for the autocorrelation within the time series.\n",
    "\n",
    "If there were no autocorrelation, then we would expect to see very random movement from one lag to the next, or trivially small correlation coefficients (the values of the y-axis are correlation coefficients!). We will see this pattern below.\n",
    "\n",
    "## The Partial Autocorrelation Function (PACF)\n",
    "\n",
    "The Partial Autocorrelation Function (PACF) provides information analogous to our ACF regarding any MA properties that our time-series may present. It is also just as easy to implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4677bd88-3b72-4373-a6f6-0b904d5c5930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "fig = plot_pacf(data['pm2.5'], lags=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63359a2-f695-4dc6-a75b-77543382c228",
   "metadata": {},
   "source": [
    "In the above figure, you can see how quickly the plot fades toward zero. This is indicative of a lack of MA processes in our current time-series. If we had seen this figure for our ACF plot, then we would have assumed that there was no AR process in our model. Let's make a table describing possible patterns:\n",
    "\n",
    "|&nbsp; | PACF | &nbsp; | \n",
    "| --- | --- | --- | \n",
    "| **ACF** |  Trails slowly to zero | Falls quickly to zero | &nbsp; |\n",
    "| Trails slowly to zero | Both AR and MA <br>processes present | AR process only\n",
    "| Falls quickly to zero | MA process only | Neither MA nor AR <br> process visible in data\n",
    "\n",
    "## Checking for Integration\n",
    "\n",
    "When checking for integration, we can use two methods: visual diagnosis or the Augmented Dickey-Fuller test. For visual diagnosis, we simply plot the variable of interest, and check whether or not there is a discernible pattern in the time-series. This is particularly easy in plotly, where we can quickly incorporate a trendline into our visual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47f3f5-6736-49bd-87fa-5805cf265b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(data[\"pm2.5\"], trendline='ols')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02880b8-3e08-44ac-8e9e-43334a0f933a",
   "metadata": {},
   "source": [
    "In this case, it is really hard to find the trendline (you'll get there if you slowly move your mouse around, though!). Once you find it, you'll see that the slope is -0.0001, which is basically 0 when you consider the scale moves from 0 to 1000. This appears to be a stationary time-series. Let's see if the statistical test agrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77f4fcb-8bae-4508-9779-d4796ebc1e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "adfuller(data['pm2.5'], maxlag=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d696a-f02e-46e0-b75f-da14e04e5055",
   "metadata": {},
   "source": [
    "According to its [documentation](https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.adfuller.html), the first number is our test statistic, which clears the 1% threshold displayed below it. The null hypothesis is that there is a unit root in our time-series (meaning it is non-stationary). \n",
    "\n",
    "We can clearly reject the null hypothesis based on our test statistic, suggesting that our data is in fact stationary. This means that we reach the same conclusion through both visual and statistical diagnosis.\n",
    "\n",
    "## Checking our model diagnosis\n",
    "\n",
    "Now that we believe that our model is stationary ($d=0$), has an AR process ($p\\geq1$), and has no MA process ($q=0$), we should construct an ARIMA(1,0,0) model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588f3d3-b4a9-4923-8ddf-fe0bbd401b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "arima = sm.tsa.ARIMA(data['pm2.5'], order=(1, 0, 0)).fit()\n",
    "arima.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0779160-d3cf-4857-835c-834376ff8a1f",
   "metadata": {},
   "source": [
    "With a fitted model, we now want to check whether or not we still see signs of AR or MA processes (we shouldn't see any integration at this stage, and we should only have to deal with it before constructing our model). The way we make this check is through the **Residual ACF** and the **Residual PACF**. These plots serve to check for the same patterns as before, but are calculated based on the model residuals rather than on the original time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737cc0f3-9079-4768-ae59-122e864662bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the residual ACF\n",
    "\n",
    "fig = plot_acf(arima.resid, lags=10)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b267242-f6e0-4317-9d95-f60afcabd87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the residual PACF\n",
    "\n",
    "fig = plot_pacf(arima.resid, lags=10)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82405b-cbe0-4038-a06a-d5755c4caf00",
   "metadata": {},
   "source": [
    "Sometimes, we will see that we need to add additional AR or MA processes to our model. This would be observed through persisting correlation between one residual and its lags in the ACF or PACF, respectively. In this case, our model seems to contain all of the appropriate processes, and so we do not need to iterate further.\n",
    "\n",
    "If we need to add AR or MA processes, we can add them one at a time, checking the residuals after each incrementation.\n",
    "\n",
    "## ARIMA with Exogenous Variables (ARIMAX)\n",
    "\n",
    "Congratulations! We can diagnose, build, and verify ARIMA models! Now we probably want to expand our model further in order to consider the influence of exogenous factors on our time-series. Very few time-series exist in a vacuum, so being able to use exogenous variables allows us to create a model that can accomodate our understanding that a time-series can be affected by many variables aside from just the past values of the time series itself.\n",
    "\n",
    "It turns out that this is a very easy transition to make! In order to include exogenous variables, we simply need to include an additional parameter in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fa16e7-0e52-4c73-8389-ac3531120d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy as pt\n",
    "\n",
    "# Using Q() because pm2.5 is a terrible column name and needs to be put in quotes\n",
    "# Also need to omit our intercept column\n",
    "y, x = pt.dmatrices(\"Q('pm2.5') ~ -1 + TEMP + PRES + Iws\", data=data)\n",
    "\n",
    "# Now we just use y as our time-series, and the argument exog=x to include our exogenous regressors\n",
    "arima = sm.tsa.ARIMA(y, order=(1, 0, 0), exog=x).fit()\n",
    "arima.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3c7acc-c742-4910-b1b3-f636abbe4280",
   "metadata": {},
   "source": [
    "By simply passing the `exog` argument with an array of exogenous data, we can include our exogenous variables in our ARIMA model. This is possible because an ARIMA model is actually just an OLS model with some new parameters: lagged values of the dependent variable. Instead of being solved algebraically, it is solved as a maximum-likelihood problem, but in essence is no different than a special case of the MLE version of OLS!\n",
    "\n",
    "# Seasonal ARIMA (SARIMA)\n",
    "\n",
    "But wait, there's more! We can also incorporate seasonal effects into our models! \n",
    "\n",
    "Why would we want to do this? Many kinds time-series data (sales and temperature, to name a few) tend to fluctuate in predictable patterns. In order to better understand our data and to make forecasts, we want to account for these fluctuations as we build our model.\n",
    "\n",
    "Let's try this with temperature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c1d6d-55b4-499e-bd37-aa07dfaa5009",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(data['TEMP'][-200:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed5f73-9342-44af-b18b-9b538c7d584e",
   "metadata": {},
   "source": [
    "When we plot the temperature data, we can clearly see temperature cycles over each day. This isn't surprising, but it IS important when generating a model to be aware of these patterns. Let's look at our ACF and PACF over a 48-lag window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d032577-13c7-449d-965c-205dd2b9a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the residual ACF\n",
    "\n",
    "fig = plot_acf(data['TEMP'], lags=48)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357daea3-63f4-43a3-9abc-81b8a6866173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the residual PACF\n",
    "\n",
    "fig = plot_pacf(data['TEMP'], lags=48)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f4dfb-d331-4360-90d3-9eb5441acdd3",
   "metadata": {},
   "source": [
    "It's easy to see that the period 24 hours in the future has a stronger correlation with the current temperature than the periods in between. In order to accomodate this kind of information, we can implement a Seasonal ARIMA model (also called SARIMA). These models can be used with our without exogenous variables (just like ARIMA).\n",
    "\n",
    "An SARIMA model has additional orders, and can be written as SARIMA$(p, d, q)(P, D, Q, S)$, where $P$ is the seasonal AR order, $D$ is the seasonal differencing order, $Q$ is the seasonal MA order, and $S$ represents the length in observations of one full cycle of seasons. In a daily temperature cycle, for example, $S$ would be 24. For monthly seasonality, $S$ would be 12 instead.\n",
    "\n",
    "Let's try out our model, incorporating an AR process, as well as a seasonal AR process with a period of 24. We can then check our residual ACF and PACF to verify the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d5cd0-a6f6-4e6c-9d93-4e419fdf7bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima = sm.tsa.statespace.SARIMAX(data['TEMP'], order = (1,0,0), seasonal_order = (1,0,0,24)).fit()\n",
    "sarima.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748dc6ed-2270-4ca2-ab7b-27763ffd86e3",
   "metadata": {},
   "source": [
    "And now our residual plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba603d2b-4727-4130-a11b-ae6e455b7f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the residual ACF\n",
    "\n",
    "fig = plot_acf(sarima.resid, lags=48)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e206f58-2b93-46b2-869d-c7c2150f1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the residual PACF\n",
    "\n",
    "fig = plot_pacf(sarima.resid, lags=48)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e603b7e-d9b6-4536-8b79-8afeb6f65cd7",
   "metadata": {},
   "source": [
    "Given the size of the correlations, our model is a borderline case now, but we will increment the order of $p$ first, and see if that makes a difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc2f90c-a21f-4628-9423-5f43a5d3a7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima = sm.tsa.statespace.SARIMAX(data['TEMP'], \n",
    "                                   order = (2,0,0), \n",
    "                                   seasonal_order = (1,0,0,24)).fit()\n",
    "sarima.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d194632-d989-44fb-b8ba-5bf436c3901d",
   "metadata": {},
   "source": [
    "Using this model to update the residual plots shows little if any progress in eliminating the remaining residual. We can also try changing $P$, the seasonal AR order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e825f347-b3eb-4123-aa60-c3e5213e6fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima = sm.tsa.statespace.SARIMAX(data['TEMP'], \n",
    "                                   order = (1,0,0), \n",
    "                                   seasonal_order = (2,0,0,24)).fit()\n",
    "sarima.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01089302-bbd0-432f-b8b8-71d5c4305683",
   "metadata": {},
   "source": [
    "When we do, we again see little difference. It appears that we have extracted about as much information as we can from our time-series, unless we have exogenous factors to include.\n",
    "\n",
    "In order to include exogenous factors in our SARIMA model, we can again use the `exog` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d8b87-b11b-4134-acb8-9dcad22f9db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This cell will NOT run quickly!\n",
    "\n",
    "y, x = pt.dmatrices(\"TEMP ~ -1 + Iws + PRES\", data=data)\n",
    "\n",
    "sarima = sm.tsa.statespace.SARIMAX(y, order = (1,0,0), seasonal_order = (1,0,0,24), exog=x).fit()\n",
    "sarima.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1060e4f-3cd8-4f5f-9799-a22ee56bc261",
   "metadata": {},
   "source": [
    "**Reading Assignment:**\n",
    "\n",
    "Think of two types of time-series data that you deal with regularly (in work or school). Explain why each data source would or would not contain seasonality, and what kind of seasonality they present if there is seasonality. Submit on Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
