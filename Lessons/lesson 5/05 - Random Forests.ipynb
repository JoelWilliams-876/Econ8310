{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b17c0a-1698-425a-8c99-7025310d9daa",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "## How does a Random Forest work?\n",
    "\n",
    "Random forests are an **ensemble** method of machine learning based on the decision tree/CART models that we have already discussed. **Ensembles** are an exciting concept within machine learning. Some models (perhaps CARTS) are very simple and fast, but tend to overfit to the data. This means that a single decision tree might learn the in-sample data very well, but struggle to make accurate predictions out-of-sample. In order to counter the tendency to overfit a model to the data, we can use an **ensemble** of models. This means that we create *many* similar models, and then aggregate their predictions to come up with a single model based on all of the lower-level models.\n",
    "\n",
    "My favorite conceptual example of an ensemble model is voting in elections. We all know people who have extreme views (in one direction or another). For the most part, those extreme views are unhelpful to society. But what if your crazy uncle got to be the only person to choose the outcome of an election? This kind of behavior used to happen (and still happens in some places), and typically results in troubling outcomes. Instead, most developed nations rely on elections to determine the form that government and laws should take. Why?\n",
    "\n",
    "We participate in democratic elections because they have a tendency to eliminate extreme outcomes. In order to gain sufficient votes to enter government or become policy, the candidate or policy must clear a significant threshold of votes. Extreme views on each end will be cancelled out by the extreme views that oppose them. Then, the sensible, and typically moderate, outcomes are favored over the more extreme outcomes. This results in more stable outcomes, both in elections and in machine learning.\n",
    "\n",
    "## Building a Random Forest\n",
    "\n",
    "In order to construct a random forest, we start with decision trees, and we make lots of them. These decision trees have several key differences from the decision trees that we might implement in isolation, though! When we create a random forest, we need to make use of **randomness** to improve our model. Each decision tree is given the following:\n",
    "1. A random sample of the available data\n",
    "2. A random sample of variables to choose from at each stage of the tree-building process\n",
    "\n",
    "Why? The random sample of data allows each tree to see the data slightly differently, and thereby increase the robustness of the overall model. The random samples of variables force each tree to choose from different variables, increasing the number of variables that are utilized in our model. This way, we don't just have a bunch of trees with the same \"world-view\". Some trees use really crappy variables, and some use the best variables (just like when you and your uncle vote!).\n",
    "\n",
    "Once we have a large forest of trees, it is time to implement the voting procedure. Each tree has been trained to make predictions on the same set of data, so we now ask each tree to make its prediction. Then, we aggregate those predictions to determine the most likely outcome based on the collective opinions of all of the tree models. In this way, the poor trees are \"voted out\", and the highly informed trees provide the basis of our predictive model.\n",
    "\n",
    "Finally, it is worth noting that building a random forest does mean a decrease in transparency. While we can still determine the variables which provide the most information to our model, we can no longer draw a single tree to present to our stakeholders. Instead, we can use representative trees from our forest, or describe the forest process. As we increase the complexity of our models, we have to work to increase the trust from stakeholders in models that are more difficult to understand.\n",
    "\n",
    "## Implementing Random Forests\n",
    "\n",
    "Random forests are computationally more complex than decision trees, but in practice are no more complicated to implement in `sklearn` than any other model. They do, however, have a few more arguments that we can use to change how they are fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5299c3c9-16cd-4e05-b942-7d3fd579fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data, then separate x and y variables\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/roomOccupancy.csv\")\n",
    "y = data['Occupancy']\n",
    "x = data[['Temperature','Humidity','Light','CO2','HumidityRatio']]\n",
    "\n",
    "# Randomly sample our data --> 70% to train with, and 30% for testing\n",
    "x, xt, y, yt = train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "# Create the model and fit it using the train data\n",
    "clf = RF(n_estimators=100, n_jobs=-1, max_depth=5)\n",
    "clf.fit(x, y)\n",
    "\n",
    "# Test our model using the testing data\n",
    "pred = clf.predict(xt)\n",
    "acc = accuracy_score(yt, pred)\n",
    "\n",
    "print(\"Model accuracy is {}%.\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af85d80-3504-4e58-921d-d5d0acde91b3",
   "metadata": {},
   "source": [
    "We break our data into training and testing sets, then implement our random forest model (imported as `RF` to make typing easier). We use `n_estimators` to specify the number of trees we want to include in our model. `n_jobs` is used to specify the number of cores in our processor that we would like to use to estimate these trees. Each tree is independent of the others, so trees can be trained in parallel wherever there are multiple processors available. Providing a value of `n_jobs=-1` tells `sklearn` to make use of all available processing cores, in order to accelerate the training process as much as possible. Note that this is likely to make it hard to do other things on your computer while the model trains.\n",
    "\n",
    "Finally, we are able to use our tree-specific arguments, such as `max_depth`, just like we did with our decision trees. These arguments will be applied to all of the trees in the forest.\n",
    "\n",
    "## Introducing MNIST\n",
    "\n",
    "The Modified National Institute of Standards and Technology database (MNIST) is a database of handwritten single-digit numbers that are frequently used as an introduction to machine vision. We can train a machine learning model to recognize patterns in an image that will help that model to discern between the possible classes that the images belong to.\n",
    "\n",
    "Each image is 28 x 28 pixels (784 variables), and has a buffer of empty pixels around the outside, so that the image is already cleaned and centered. The best way to understand the data, though, is to visualize some of it. Let's load a [sample of the data](https://github.com/dustywhite7/pythonMikkeli/blob/master/exampleData/mnistTrain.csv?raw=true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e577d2-136a-42bc-9049-83c65622111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting library, numpy to shape our data\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "# Read in the MNIST sample\n",
    "mnist = pd.read_csv(\"https://github.com/dustywhite7/pythonMikkeli/blob/master/exampleData/mnistTrain.csv?raw=true\")\n",
    "\n",
    "# Randomly choose a row, ignore first column (the label), transform into an array\n",
    "image = np.asarray(mnist.iloc[np.random.randint(1000),1:])\n",
    "\n",
    "# Reshape the data into a 28x28 square\n",
    "image = np.reshape(image, (28,28))\n",
    "\n",
    "# Render the image\n",
    "px.imshow(image, color_continuous_scale ='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1957b6d2-0f0e-4cdb-8d56-d886311d488b",
   "metadata": {},
   "source": [
    "With the MNIST dataset, we can test our ability to identify/classify handwritten digits using algorithms ranging from Decision Trees to Random Forests to neural networks! In this class, we will keep the data in a flat structure (with one row of 784 pixel variables for each image) in order to allow our models to try to classify which of the ten integers each image represents. \n",
    "\n",
    "This well-known dataset is both challenging and engaging, since it forms the basis for the kinds of models that are necessary to identify other items in images (such as people), and that can provide the inputs required for such use cases as self-driving cars or identifying farm yields based on satellite images!\n",
    "\n",
    "**Reading Reflection**:\n",
    "\n",
    "Create a Random Forest model that predicts the handwritten digit from the [MNIST Dataset](https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/mnistTrain.csv). What accuracy are you able to reach? What changes did you have to make to your model in order to reach that accuracy level? Why do you think that those changes improved your accuracy? Submit your answer in Canvas. Feel free to discuss your results with your classmates!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
