{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00525896-0060-40e2-b269-75122de31845",
   "metadata": {
    "panel-layout": {
     "height": 93.86250305175781,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# Boosted Tree Models\n",
    "\n",
    "## Another ensemble\n",
    "\n",
    "Remember from last lesson that an **ensemble** is a collection of models that are combined under a single decision mechanism to model a specific outcome of interest. Our first example, **Random Forests**, is designed as an analogue to computer democracy. It improves the accuracy of the **Decision Tree** model by using a **bagging** (bootstrap aggregation) technique in addition to randomization. These randomized trees then create a more complete picture of the decision space, enabling a Random Forest to improve on the accuracy of a Decision Tree while also reducing the likelihood of overfitting.\n",
    "\n",
    "This week, our model will not rely on the bagging technique. Instead, we will use **boosting** to improve the accuracy of our model while sticking with a tree-based model. As we will see, this will often improve performance to a greater extent than bagging or Random Forest models, but does not insulate against overfitting.\n",
    "\n",
    "## Understanding Boosting\n",
    "\n",
    "How is boosting different than bagging? Where bagging focuses on simultaneous and distinct models, boosting is focused on stacking models, so that one model provides input to the next model, and the process continues through all of the ensemble models. Let's take a closer look at how this can actually benefit us.\n",
    "\n",
    "### The math\n",
    "\n",
    "We can start off by describing what we aim to accomplish. Our goal with any machine learning algorithm is to estimate the underlying functional form ($f(x)$) of our variable of interest. The true functional form can't be known unless we fully describe the data generating process (unlikely), so instead, we build a model of that function based on observable characteristics. Let's start with a one stage model building our estimator of $f(x)$, which we will write as $\\hat{f}_M(x)$, where $M$ is the stage of our model.\n",
    "\n",
    "$$ \\hat{f}_0(x) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9964f31-39c2-4ef4-99e0-733531eac3b9",
   "metadata": {},
   "source": [
    "Well, that's not very interesting...\n",
    "\n",
    "Why is our 0-stage model just 0? To create a boosting algorithm, each individual stage in our model is constructed with the aim of explaining the error in the previous stage's model. In order for our 1-stage model to be able to fit this mold, we build our 0-stage, or baseline, model as a uniform predictor. It will simply make the same prediction independent of the value of $x$. Our first stage model is then designed to predict the error in the 0-stage model.\n",
    "\n",
    "$$ \\hat{f}_1(x) = \\hat{f}_0(x) - 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bcd1d9-952e-47d0-ad2a-ef3385171b5c",
   "metadata": {},
   "source": [
    "We can then write our one stage model as\n",
    "\n",
    "$$ \\hat{f}(x) = \\hat{f}_0(x) - \\hat{f}_1(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5dd46-344c-4d05-a329-95a17080f106",
   "metadata": {},
   "source": [
    "If $M$ > 1, we simply continue to add stages, so that at stage $m$:\n",
    "\n",
    "$$ \\hat{f}_m(x) = \\hat{f}_{m-1}(x) - \\hat{f}^*_m(x) $$\n",
    "\n",
    "where $\\hat{f}^*_m(x)$ is the current prediction of last stage's error term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8636ca4d-9255-4207-9c53-941869eae23b",
   "metadata": {},
   "source": [
    "### Visually\n",
    "\n",
    "Let's look at a simple visual example found [here](https://developers.google.com/machine-learning/decision-forests/intro-to-gbdt). The visual below contains three panels. The leftmost panel is our current \"complete\" model, comprising all of our previous models for relevant values of $x$. In stage 0, though, this is just a straight line, since our model is na√Øve. The second panel is a plot of the current error term across values of $x$. The third panel represents the current stage's tree-based model of the error term.\n",
    "\n",
    "Together, these plots represent the process of fitting the next stage in a boosted tree model. We take our current model, map out the error of that model, and then fit a new model to the current error \"function\".\n",
    "\n",
    "![](https://developers.google.com/static/machine-learning/decision-forests/images/ThreePlotsAfterFirstIteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6494eeea-19ca-4fe3-9809-a12ef82ad1c8",
   "metadata": {},
   "source": [
    "We can see even more clearly what is happening when we look at this figure in combination with the same figure for the *next* stage. Now, our right-hand panel/model has been flipped upside down and combined with our previous \"complete\" model (this is why we wrote the \"additive\" nature of our boosted tree model as subtraction rather than as addition) to form the left panel of the next stage. Again, the middle panel is the current stage's error function, and the right panel is a tree-based model of the current error.\n",
    "\n",
    "![](https://developers.google.com/static/machine-learning/decision-forests/images/ThreePlotsAfterSecondIteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa640eba-4a86-4141-b565-a394787924ca",
   "metadata": {},
   "source": [
    "Between the right panel of the 0 stage model, and the left panel of the stage 1 model, we can see that the predictions of our outcome of interest are simply the flipped predictions of the errors from the previous stage. Why? Because the error is how far off we were last time. If we don't want to be that far off, we should simply subtract the error from our past predictions from our those predictions to get a better set of predictions. The error term in stage 1 is smaller than in stage 0 (we've corrected some of the stage 0 model's biggest errors), and so the additive component in our new model (the right panel) is now correcting the largest remaining errors.\n",
    "\n",
    "When we advance to the next stage, we will see that the right hand panel will be combined (upside down) with the left panel to create the new model to start the next stage. This will continue throughout each stage of our boosted model, which slowly gains in accuracy as we compound each of the simple error-prediction models to create our true model of the data generation process.\n",
    "\n",
    "![](https://developers.google.com/static/machine-learning/decision-forests/images/ThreePlotsAfterTenIterations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c9ccb-05d9-4b5f-adc8-a6c4dec6f8e3",
   "metadata": {},
   "source": [
    "In this particular example, the stage 10 model actually closely resembles the overall data-generation process that we could observe as the error term in stage 0. While it is blocky and not smooth, it is shaped in much the same way as the wave function it was meant to predict. This same process will hold as we use boosted tree models for continuous dependent variables (as we did above), as well as when we attempt to predict outcomes in a classification context, as we will do in our example below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0828cec5-df3e-4b44-a03c-648237426b1c",
   "metadata": {},
   "source": [
    "## Implementing boosting with `xgboost`\n",
    "\n",
    "To implement our boosted tree model, we are going to use the `xgboost` library. It's a nice library for a few reasons. First, it's available in multiple languages (R, Python, Julia, etc.) so you can learn one model and then be able to use it wherever you find yourself. Second, the `xgboost` model has been carefully optimized to run as efficiently as possible, including being able to take advantage of GPUs or parallel computation when training larger models.\n",
    "\n",
    "To get started, let's load our libraries and the MNIST training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d71cfd7-3f34-418c-9435-53468a600e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8310/raw/master/DataSets/mnistTrain.csv\")\n",
    "\n",
    "# Upper case before split, lower case after\n",
    "Y = data['Label']\n",
    "# make sure you drop a column with the axis=1 argument\n",
    "X = data.drop('Label', axis=1) \n",
    "\n",
    "x, xt, y, yt = train_test_split(X, Y, test_size=0.1,\n",
    "     random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb474933-0f88-4fa8-8d7d-12731b27a0b7",
   "metadata": {},
   "source": [
    "Our data has ten classes, so we are doing classification, but we need to be able to predict more than just \"success\" or \"failure\". Any integer between 0 and 9 is a valid prediction.\n",
    "\n",
    "Let's prepare our model accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45380ffb-c991-43ef-aa8b-3a8d127d9cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you need to install xgboost\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "717ea658-8bc1-4072-84bd-d0b115b10870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=50, \n",
    "    max_depth=3,\n",
    "    learning_rate=0.5, \n",
    "    objective='multi:softmax'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72d5b4-7c07-416f-b46b-df6736c22786",
   "metadata": {},
   "source": [
    "Some of these arguments will look familiar, because `xgboost` tries to adhere as much as possible to the `sklearn` API patterns. Just like our Random Forests, we specify `n_estimators` for our ensemble model. In this case, this value does not represent the number of parallel trees that will be voting. It instead represents the number of stages or iterations to which our model will be subjected.\n",
    "\n",
    "`max_depth` specifies the maximum depth of the tree at each stage that will be used to model the current error function.\n",
    "\n",
    "`learning_rate` is a weight that is applied to each stage in an attempt to prevent overfitting. A learning rate of less than one reduces the impact of a given stage's weight on the overall model, so that the model converges more slowly toward the underlying function, and the chance of overshooting is reduced.\n",
    "\n",
    "Fitting the model is just as simple as it was for our `sklearn` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3a1f2a3-3fe5-45e0-8e30-7ab407a458a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 93.40%\n"
     ]
    }
   ],
   "source": [
    "# Fit to our training split\n",
    "xgb.fit(x, y)\n",
    "\n",
    "# Make predictions based on the testing x values\n",
    "pred = xgb.predict(xt)\n",
    "\n",
    "# Print out the accuracy score\n",
    "print(f\"Accuracy score: {accuracy_score(yt, pred)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d0b93-04f8-4ff0-b782-cf61e530a341",
   "metadata": {},
   "source": [
    "A very nice accuracy level for such a simple model! From here, we can explore whether other combinations of hyperparamters increase our model performance or not. In order to do that, we should spend some time talking about cross validation.\n",
    "\n",
    "# Cross-Validation\n",
    "\n",
    "In order to maximize the information that we have about our model's performance, we will often test the model on many draws of our existing data. We do this because the random samples that we draw from our data can actually have a significant impact on how we interpret our model's performance. Some draws will have outliers in the training set, so that we build a model that accomodates them, while other draws may leave outliers only in our testing draw. A process of taking multiple samples to explore how our model performs on different subsets of data is called **cross-validation**. We resample our training and testing data $k$ times, and compare the performance of the models, so that we can gain a better picture of how sensitive our model may be to including or excluding different draws of data.\n",
    "\n",
    "If the models for each of our draws perform more or less equally well, then we can treat our model as well-specified. If not, then we know performance was sample-dependent.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "We often call cross-validation \"$k$-fold cross-validation\", where $k$ is the number of groups into which we will sample our data. The image below (from the sklearn documentation) depicts a 5-fold cross-validation. Steps to accomplish this are as follows:\n",
    "\n",
    "1) Sample data into training and testing data. The testing data will not be involved in the cross-validation, since it will be used as a final validity check on our model **after** we do our cross-validation.\n",
    "2) In our training data, we split our data into $k$ (in this case, 5) folds. Each fold will be of equal size, and each observation is only drawn once. We will use each observation in our training data during this exercise.\n",
    "3) Once we have all of our folds, we run our model $k$ times. In the first iteration, we take $k-1$ folds of our data as training data, with the remaining fold serving as our testing data. We train the model, then test and store our model and performance metrics.\n",
    "4) Repeat step (3), withholding each of the folds once in turn.\n",
    "5) Compare the performance of the models.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    "\n",
    "In the case of cross-validation, we are not looking for the BEST model. We are instead looking for consistency across models. A well specified model should perform similarly in each instance, and we want to be able to minimize the variation across our folds. By doing this, we ensure that we have an accurate perception of how our model will perform when it encounters new data.\n",
    "\n",
    "If we are not happy with our models performance, we can try a new model and run cross-validation again. If we are satisfied, then we can move on to training our model on the entire training dataset. Having done so, we are ready to test our model against the real test data, where we should expect our model to once again perform at the same level as it did during cross-validation. If it does not, this can serve as another red flag indicating overfitting.\n",
    "\n",
    "Below is some example code of reporting accuracy of models within a cross-validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24171a-dc21-43c8-b8c0-ab165555d7b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# If we have imported data and created x, y already:\n",
    "kf = KFold(n_splits=10) # 10 \"Folds\"\n",
    "\n",
    "models = [] # We will store our models here\n",
    "\n",
    "for train, test in kf.split(x): # Iterate over folds\n",
    "    model = XGBClassifier(\n",
    "            n_estimators=50, \n",
    "            max_depth=3,\n",
    "            learning_rate=0.5, \n",
    "            objective='multi:softmax'\n",
    "                ).fit(x.values[train], y.values[train]) # Fit model\n",
    "    accuracy = accuracy_score(y.values[test],    # Store accuracy\n",
    "        model.predict(x.values[test]))\n",
    "    print(\"Accuracy: \", accuracy)            # Print results\n",
    "    models.append([model, accuracy])      # Store it all\n",
    "\n",
    "print(\"Mean Model Accuracy: \", np.mean([model[1] for model in models]))\n",
    "print(\"Model Accuracy Standar Deviation: \", np.std([model[1] for model in models]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced6bd21-685d-46d1-b5de-29fe93009429",
   "metadata": {},
   "source": [
    "**Reading Reflection**:\n",
    "\n",
    "Are you more comfortable with the tradeoffs of random forests or boosted trees? Explain your preference in 2-3 sentences.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "panel-cell-order": [
   "00525896-0060-40e2-b269-75122de31845"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
